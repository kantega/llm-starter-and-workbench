langchain4j.openai.chat-model.temperature=0.5
langchain4j.openai.timeout=60s

langchain4j.open-ai.chat-model.log-requests=true
langchain4j.open-ai.chat-model.log-responses=true

langchain4j.ollama.chat-model.temperature=0.5
langchain4j.ollama.timeout=60s

langchain4j.ollama.chat-model.log-requests=true
langchain4j.ollama.chat-model.log-responses=true

#quarkus.log.category."dev.langchain4j".level=DEBUG
#quarkus.log.category."dev.ai4j.openai4j".level=DEBUG

quarkus.rest-client.ollama-api.url=http://localhost:11434/

llmwb.ollama.embedding-model.names=nomic-embed-text,mxbai-embed-large
llmwb.openai.chat-model.names=gpt-3.5-turbo,gpt-3.5-turbo-16k,gpt-4,gpt-4-turbo-preview,gpt-4-32k
